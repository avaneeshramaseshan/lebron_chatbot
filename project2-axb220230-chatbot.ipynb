{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8266707,"sourceType":"datasetVersion","datasetId":4907549}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\n# Import necessary libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T21:38:12.994638Z","iopub.execute_input":"2024-04-29T21:38:12.994890Z","iopub.status.idle":"2024-04-29T21:38:15.388865Z","shell.execute_reply.started":"2024-04-29T21:38:12.994866Z","shell.execute_reply":"2024-04-29T21:38:15.387892Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\nurl = '/kaggle/input/lebrontruedata/lebronTrueData.csv'  # Change this to the path of your uploaded file\ndf = pd.read_csv(url)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:38:18.871470Z","iopub.execute_input":"2024-04-29T21:38:18.872296Z","iopub.status.idle":"2024-04-29T21:38:18.900930Z","shell.execute_reply.started":"2024-04-29T21:38:18.872255Z","shell.execute_reply":"2024-04-29T21:38:18.900033Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(len(data))\nprint(data[:2])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:38:23.622062Z","iopub.execute_input":"2024-04-29T21:38:23.622426Z","iopub.status.idle":"2024-04-29T21:38:23.639398Z","shell.execute_reply.started":"2024-04-29T21:38:23.622398Z","shell.execute_reply":"2024-04-29T21:38:23.638638Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"828\n   index                                          question  \\\n0      1   What team does LeBron James currently play for?   \n1      2  How many NBA championships has LeBron James won?   \n\n                                              answer  \n0  LeBron James is currently playing for the Los ...  \n1       LeBron James has won four NBA championships.  \n","output_type":"stream"}]},{"cell_type":"code","source":"questions = df['question']\nanswers = df['answer']\nprint(questions[:2])\nprint(answers[:2])\nprint(\"Data size: \", len(df))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:38:32.013090Z","iopub.execute_input":"2024-04-29T21:38:32.013654Z","iopub.status.idle":"2024-04-29T21:38:32.020048Z","shell.execute_reply.started":"2024-04-29T21:38:32.013623Z","shell.execute_reply":"2024-04-29T21:38:32.019098Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0     What team does LeBron James currently play for?\n1    How many NBA championships has LeBron James won?\nName: question, dtype: object\n0    LeBron James is currently playing for the Los ...\n1         LeBron James has won four NBA championships.\nName: answer, dtype: object\nData size:  828\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vectorizing the text data using TF-IDF\ntfidf_vectorizer = TfidfVectorizer()  # Create a TF-IDF Vectorizer object\nX = tfidf_vectorizer.fit_transform(questions)  # Transform questions to a matrix of TF-IDF features\n\n# Since MLPRegressor predicts continuous outputs, we will use the index of the answers as the target variable.\ny = range(len(answers))  # Create numeric labels for each answer\n\n# Splitting the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% training, 20% testing","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:38:41.390729Z","iopub.execute_input":"2024-04-29T21:38:41.391603Z","iopub.status.idle":"2024-04-29T21:38:41.447157Z","shell.execute_reply.started":"2024-04-29T21:38:41.391571Z","shell.execute_reply":"2024-04-29T21:38:41.446290Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Printing shapes of the datasets to verify\nprint(\"Training set shape:\", X_train.shape)\nprint(\"Test set shape:\", X_test.shape)\nprint(\"Training labels:\", y_train)\nprint(\"Test labels:\", y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:38:56.819646Z","iopub.execute_input":"2024-04-29T21:38:56.820081Z","iopub.status.idle":"2024-04-29T21:38:56.825472Z","shell.execute_reply.started":"2024-04-29T21:38:56.820050Z","shell.execute_reply":"2024-04-29T21:38:56.824629Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training set shape: (662, 1021)\nTest set shape: (166, 1021)\nTraining labels: [530, 715, 346, 696, 599, 302, 631, 79, 148, 765, 196, 653, 338, 685, 323, 198, 133, 549, 227, 506, 55, 487, 626, 610, 404, 668, 552, 209, 309, 664, 60, 595, 393, 363, 582, 462, 593, 757, 587, 213, 311, 746, 90, 776, 181, 763, 380, 811, 158, 69, 588, 131, 44, 281, 70, 672, 567, 326, 423, 135, 82, 165, 164, 28, 426, 193, 737, 538, 333, 136, 826, 434, 140, 494, 6, 542, 698, 235, 649, 73, 670, 275, 465, 145, 306, 234, 220, 818, 332, 132, 396, 327, 445, 41, 514, 108, 360, 56, 433, 294, 429, 24, 478, 367, 382, 51, 659, 760, 264, 545, 576, 350, 299, 18, 603, 286, 83, 61, 482, 285, 507, 398, 467, 182, 328, 519, 223, 443, 594, 334, 344, 601, 513, 176, 768, 731, 163, 248, 536, 733, 74, 730, 424, 104, 114, 314, 92, 490, 89, 450, 761, 770, 597, 532, 94, 11, 804, 43, 42, 167, 741, 516, 634, 431, 178, 548, 557, 177, 584, 578, 257, 375, 335, 493, 15, 3, 351, 256, 388, 409, 714, 499, 436, 755, 495, 324, 477, 9, 249, 22, 221, 623, 713, 812, 789, 340, 693, 203, 237, 93, 568, 712, 526, 284, 184, 671, 762, 153, 75, 483, 521, 277, 68, 531, 188, 271, 522, 236, 88, 704, 117, 125, 682, 289, 238, 0, 819, 572, 723, 395, 479, 486, 278, 667, 116, 228, 585, 439, 764, 274, 318, 569, 144, 596, 525, 753, 604, 369, 268, 586, 307, 310, 640, 46, 349, 371, 541, 261, 195, 827, 694, 107, 59, 618, 547, 100, 718, 744, 602, 793, 179, 304, 448, 497, 798, 149, 124, 656, 720, 185, 559, 50, 500, 446, 817, 805, 321, 353, 766, 142, 370, 141, 399, 263, 320, 19, 172, 675, 312, 390, 772, 12, 407, 408, 305, 354, 25, 616, 169, 38, 175, 245, 298, 690, 416, 783, 272, 473, 651, 154, 126, 449, 590, 341, 430, 287, 113, 518, 173, 359, 781, 57, 570, 222, 711, 280, 17, 127, 322, 255, 669, 470, 617, 468, 796, 190, 115, 725, 180, 301, 637, 773, 609, 579, 794, 517, 813, 45, 752, 157, 628, 171, 16, 511, 48, 802, 515, 580, 480, 283, 554, 225, 26, 652, 437, 364, 229, 37, 797, 374, 469, 799, 705, 611, 194, 707, 716, 503, 801, 691, 800, 162, 717, 152, 575, 591, 633, 111, 226, 581, 103, 421, 419, 119, 53, 151, 403, 780, 207, 822, 809, 8, 673, 36, 452, 253, 303, 625, 571, 598, 606, 262, 297, 414, 150, 658, 735, 550, 488, 147, 146, 641, 807, 620, 348, 463, 325, 186, 123, 706, 143, 791, 197, 279, 293, 400, 122, 183, 202, 438, 246, 415, 734, 129, 402, 678, 644, 754, 219, 756, 630, 759, 624, 697, 638, 386, 803, 509, 267, 788, 441, 496, 112, 232, 721, 607, 373, 233, 655, 317, 410, 743, 358, 258, 627, 632, 282, 376, 384, 224, 786, 679, 472, 347, 505, 639, 816, 767, 748, 619, 708, 645, 739, 556, 577, 85, 242, 159, 524, 35, 540, 170, 654, 710, 751, 779, 95, 563, 240, 574, 460, 553, 727, 206, 392, 397, 703, 217, 4, 642, 742, 612, 546, 683, 98, 573, 406, 502, 47, 32, 200, 134, 27, 749, 230, 489, 378, 288, 418, 674, 391, 592, 498, 138, 62, 471, 647, 128, 806, 520, 64, 14, 156, 40, 492, 379, 187, 216, 52, 337, 719, 724, 295, 701, 251, 726, 461, 455, 825, 269, 201, 161, 555, 729, 401, 702, 476, 105, 565, 389, 1, 774, 561, 80, 205, 34, 508, 427, 454, 366, 91, 339, 564, 345, 241, 13, 315, 600, 387, 273, 166, 821, 646, 484, 810, 504, 243, 566, 562, 686, 189, 782, 699, 475, 681, 510, 58, 474, 560, 747, 252, 21, 313, 459, 160, 276, 191, 385, 413, 491, 343, 769, 308, 661, 130, 663, 99, 372, 87, 458, 330, 214, 466, 121, 614, 20, 700, 71, 106, 270, 435, 102]\nTest labels: [608, 457, 290, 558, 168, 684, 814, 86, 260, 680, 750, 137, 265, 215, 30, 381, 247, 259, 775, 544, 485, 63, 210, 648, 239, 451, 109, 512, 736, 342, 383, 356, 139, 368, 39, 551, 481, 815, 67, 65, 534, 790, 23, 635, 442, 66, 787, 665, 660, 199, 695, 687, 411, 250, 543, 539, 732, 636, 583, 456, 824, 778, 688, 96, 677, 208, 440, 377, 464, 425, 728, 49, 362, 244, 78, 784, 204, 823, 33, 31, 808, 535, 615, 212, 218, 447, 211, 72, 77, 120, 528, 300, 365, 155, 291, 412, 355, 266, 527, 319, 785, 745, 650, 174, 5, 54, 777, 254, 589, 820, 97, 394, 76, 629, 643, 792, 336, 621, 444, 758, 84, 10, 231, 110, 296, 692, 292, 662, 331, 118, 657, 622, 29, 501, 453, 422, 417, 81, 420, 316, 523, 605, 709, 329, 666, 428, 537, 689, 529, 676, 7, 795, 738, 101, 361, 352, 533, 405, 2, 740, 432, 613, 192, 357, 722, 771]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the MLP Regressor\nmlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n# Train the MLP Regressor on the training data\nmlp.fit(X_train, y_train)\n\n# Initialize the Naive Bayes Classifier\nnb = MultinomialNB()\n# Since Naive Bayes expects non-negative integers, we use the fit_transform method to ensure this\nX_train_nb = tfidf_vectorizer.fit_transform(questions)  # Re-vectorize for NB as it requires non-negative feature values\ny_train_nb = y  # Targets remain the same\n# Train Naive Bayes on the data\nnb.fit(X_train_nb, y_train_nb)\n\n# Making predictions with both models on the test data\n# For MLP, we'll predict the index of the closest answer\nmlp_predictions = mlp.predict(X_test)\nmlp_predictions = [int(round(index)) for index in mlp_predictions]  # Round the continuous outputs to get the nearest integer index\n\n# For Naive Bayes, directly predict the index\nnb_predictions = nb.predict(tfidf_vectorizer.transform([q for i, q in enumerate(questions) if i in y_test]))  # Transform test questions for prediction\n\n# Evaluate the models\n# We will use accuracy as the metric since we are predicting the index of answers\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\nnb_accuracy = accuracy_score(y_test, nb_predictions)\n\n# Printing accuracies to compare the models\nprint(\"MLP Regressor Accuracy:\", mlp_accuracy)\nprint(\"Naive Bayes Accuracy:\", nb_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:39:08.032738Z","iopub.execute_input":"2024-04-29T21:39:08.033435Z","iopub.status.idle":"2024-04-29T21:39:24.339337Z","shell.execute_reply.started":"2024-04-29T21:39:08.033406Z","shell.execute_reply":"2024-04-29T21:39:24.338347Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"MLP Regressor Accuracy: 0.0\nNaive Bayes Accuracy: 0.006024096385542169\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train model with 100% of the data and get accuracy again\nmlp.fit(X, y)\n\n# For MLP, we'll predict the index of the closest answer\nmlp_predictions = mlp.predict(X_test)\nmlp_predictions = [int(round(index)) for index in mlp_predictions]  # Round the continuous outputs to get the nearest integer index\n\n# Evaluate the model\nmlp_accuracy = accuracy_score(y_test, mlp_predictions)\n\n# Printing accuracies to compare the models\nprint(\"MLP Regressor Accuracy with 100% data:\", mlp_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:39:38.532309Z","iopub.execute_input":"2024-04-29T21:39:38.533030Z","iopub.status.idle":"2024-04-29T21:39:58.153373Z","shell.execute_reply.started":"2024-04-29T21:39:38.533000Z","shell.execute_reply":"2024-04-29T21:39:58.152406Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"MLP Regressor Accuracy with 100% data: 0.03614457831325301\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nsecret_value_1 = user_secrets.get_secret(\"lebron_chatbot\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:43:41.688380Z","iopub.execute_input":"2024-04-29T21:43:41.689019Z","iopub.status.idle":"2024-04-29T21:43:41.961102Z","shell.execute_reply.started":"2024-04-29T21:43:41.688990Z","shell.execute_reply":"2024-04-29T21:43:41.960374Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n!pip install accelerate -U\n!pip install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T21:43:55.667134Z","iopub.execute_input":"2024-04-29T21:43:55.668030Z","iopub.status.idle":"2024-04-29T21:44:33.296714Z","shell.execute_reply.started":"2024-04-29T21:43:55.667993Z","shell.execute_reply":"2024-04-29T21:44:33.295569Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.29.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport requests\nfrom bs4 import BeautifulSoup\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nimport json\nimport os\n\nclass Chatbot:\n    def __init__(self):\n        self.user_profile_path = '/kaggle/working/user_profiles/'\n        self.ensure_directory_exists(self.user_profile_path)\n        self.user_profile = {}\n        self.load_or_initialize_user_profile()\n        self.vectorizer = tfidf_vectorizer  # Make sure this is defined somewhere or passed as an argument\n        self.tokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/working/fine_tuned_gpt2_latest\")\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = 'left'\n        self.model = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/fine_tuned_gpt2_latest\")\n        if not self.user_profile.get('name'):\n            self.set_user_name()\n        else:\n            print(f\"Welcome back, {self.user_profile['name']}! What can I do for you today?\")\n    \n    def ensure_directory_exists(self, path):\n        if not os.path.exists(path):\n            os.makedirs(path)\n    \n    def load_or_initialize_user_profile(self):\n        name = input(\"Welcome to the LeBron James Chatbot! What's your name? \")\n        profile_path = f\"{self.user_profile_path}{name}.json\"\n        if os.path.exists(profile_path):\n            with open(profile_path, 'r') as file:\n                self.user_profile = json.load(file)\n            print(f\"Welcome back, {name}! What can I do for you today?\")\n        else:\n            self.user_profile = {'name': name}\n            self.set_user_preferences()\n    \n    def set_user_preferences(self):\n        # Gather additional user preferences\n        self.user_profile['favorite_team'] = input(\"What's your favorite basketball team? \").strip()\n        self.user_profile['favorite_player'] = input(\"Who's your favorite player? \").strip()\n        self.save_user_profile()\n\n    def save_user_profile(self):\n        profile_path = f\"{self.user_profile_path}{self.user_profile['name']}.json\"\n        with open(profile_path, 'w') as file:\n            json.dump(self.user_profile, file)\n   \n    def compare_answers(self, answer1, answer2):\n        # Compute cosine similarity between two answers\n        answers_vector = self.vectorizer.transform([answer1, answer2])\n        similarity = cosine_similarity(answers_vector[0:1], answers_vector[1:2])[0][0]\n        return similarity\n    \n    def ask_bot(self, question):\n        # Handling user profile updates based on specific questions\n        if \"my name is\" in question.lower():\n            self.user_profile['name'] = question.split(\"my name is\")[-1].strip()\n            return f\"Nice to meet you, {self.user_profile['name']}!\"\n        if \"my favorite team is\" in question.lower():\n            self.user_profile['favorite_team'] = question.split(\"my favorite team is\")[-1].strip()\n            return f\"Great, I'll remember that your favorite team is {self.user_profile['favorite_team']}.\"\n        if \"my favorite player is\" in question.lower():\n            self.user_profile['favorite_player'] = question.split(\"my favorite player is\")[-1].strip()\n            return f\"Awesome, I've noted that your favorite player is {self.user_profile['favorite_player']}.\"\n        \n        # Predict answers from both MLP and Naive Bayes\n        question_vector = self.vectorizer.transform([question])\n        \n        # Get predictions as indices\n        mlp_index = int(mlp.predict(question_vector)[0])\n        nb_index = int(nb.predict(question_vector)[0])\n        \n        # Fetch the actual answers from the answers list using these indices\n        mlp_answer = answers[mlp_index]\n        nb_answer = answers[nb_index]\n        \n        # Encode the inputs, including attention mask\n        encoded_input = self.tokenizer.encode_plus(\n            question,\n            return_tensors='pt',\n            max_length=512,  # Ensure this is enough to handle your input question length\n            truncation=True,\n            padding='max_length'\n        )\n        input_ids = encoded_input['input_ids']\n        attention_mask = encoded_input['attention_mask']\n\n        # Generate a response using the fine-tuned model\n        outputs = self.model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=512,  # Total length of output text (input + generated)\n            max_new_tokens=100,  # Maximum number of new tokens to generate\n            pad_token_id=self.tokenizer.eos_token_id\n        )\n        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Compare answers for similarity\n        similarity = self.compare_answers(mlp_answer, nb_answer)\n        \n        if similarity > 0.3:\n            # If similarity is high, choose one (arbitrarily choosing MLP here)\n            selected_answer = mlp_answer\n        else:\n            # If low similarity, use GPT-2\n            encoded_input = self.tokenizer.encode_plus(\n                question,\n                return_tensors='pt',\n                max_length=512,\n                truncation=True,\n                padding='max_length'\n            )\n            input_ids = encoded_input['input_ids']\n            attention_mask = encoded_input['attention_mask']\n\n            outputs = self.model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                max_length=512,\n                max_new_tokens=100,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n            gpt2_answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            # Check if GPT-2 answer is valid; if not, do web lookup\n            if len(gpt2_answer) < 20:\n                web_answer = self.web_lookup(question)\n                if \"wiki\" not in web_answer.lower():\n                    selected_answer = web_answer\n            else:\n                selected_answer = gpt2_answer\n\n        return selected_answer\n\n    def web_lookup(self, question):\n        url = f'https://www.google.com/search?q={question}'\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n        response = requests.get(url, headers=headers)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Attempt to find more detailed snippets\n        results = soup.find_all('div', class_='tF2Cxc')  # Google's class for a result container, this may change over time\n        if not results:\n            return soup.find('div', class_='BNeawe').text\n\n        detailed_answers = [result.find('span', class_='aCOpRe').text for result in results if result.find('span', class_='aCOpRe')]\n        snippets = soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd')  # Google's class for description snippets\n        if snippets:\n            return ' '.join(snippet.text for snippet in snippets[:2])  # Return the first two snippets combined\n        if detailed_answers:\n            return ' '.join(detailed_answers[:3])  # Return the first three detailed answers combined\n        else:\n            return soup.find('div', class_='BNeawe').text\n\n\n    def run(self):\n        while True:\n            user_input = input(\"You: \")\n            if user_input.lower() == 'exit':\n                print(\"Exiting the chatbot. Goodbye!\")\n                break\n            response = self.ask_bot(user_input)\n            print(\"Chatbot:\", response)\n\n# Start the chatbot\nchatbot = Chatbot()\nchatbot.run()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T23:58:29.240729Z","iopub.execute_input":"2024-04-29T23:58:29.241070Z","iopub.status.idle":"2024-04-29T23:58:56.863621Z","shell.execute_reply.started":"2024-04-29T23:58:29.241044Z","shell.execute_reply":"2024-04-29T23:58:56.862122Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdin","text":"Welcome to the LeBron James Chatbot! What's your name?  Ashwin\n"},{"name":"stdout","text":"Welcome back, Ashwin! What can I do for you today?\nWelcome back, Ashwin! What can I do for you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  What team does LeBron James play for?\n"},{"name":"stderr","text":"Both `max_new_tokens` (=100) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\nBoth `max_new_tokens` (=100) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"Chatbot: What team does LeBron James play for? [SEP] LeBron James plays for the Cleveland Cavaliers.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"Exiting the chatbot. Goodbye!\n","output_type":"stream"}]}]}